{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4350b3-de65-4564-9d90-75c1ea0da996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/jyim/miniconda3/envs/dev/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /data/rsg/chemistry/jyim/miniconda3/envs/dev/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "blk = lambda ic, oc: nn.Sequential(\n",
    "    nn.Conv2d(ic, oc, 7, padding=3),\n",
    "    nn.BatchNorm2d(oc),\n",
    "    nn.LeakyReLU(),\n",
    ")\n",
    "\n",
    "\n",
    "def residual_resample(weights):\n",
    "    \"\"\"residual_resample samples from discrete distribution with probabilities `weights'\n",
    "    trying to maintain diversity.\n",
    "\n",
    "    Args:\n",
    "        weights: simplex variable weights of shape [B]\n",
    "\n",
    "    Returns:\n",
    "        idcs of samples\n",
    "    \"\"\"\n",
    "    B = len(weights)\n",
    "    weights *= B/sum(weights)\n",
    "\n",
    "    weights_floor = np.floor(weights)\n",
    "    weights_remainder = weights - weights_floor\n",
    "    idcs_no_replace = sum([[i]*int(w) for i, w in enumerate(weights_floor)], [])\n",
    "\n",
    "    N_replace = sum(weights_remainder)\n",
    "    N_replace = int(np.round(N_replace))\n",
    "    idcs_replace = np.random.choice(B, size=N_replace, p=weights_remainder/sum(weights_remainder))\n",
    "    idcs = idcs_no_replace + list(idcs_replace)\n",
    "    return idcs, N_replace\n",
    "\n",
    "\n",
    "class DummyEpsModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This should be unet-like, but let's don't think about the model too much :P\n",
    "    Basically, any universal R^n -> R^n model should work.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channel: int) -> None:\n",
    "        super(DummyEpsModel, self).__init__()\n",
    "        self.conv = nn.Sequential(  # with batchnorm\n",
    "            blk(n_channel, 64),\n",
    "            blk(64, 128),\n",
    "            blk(128, 256),\n",
    "            blk(256, 512),\n",
    "            blk(512, 256),\n",
    "            blk(256, 128),\n",
    "            blk(128, 64),\n",
    "            nn.Conv2d(64, n_channel, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t) -> torch.Tensor:\n",
    "        # Lets think about using t later. In the paper, they used Tr-like positional embeddings.\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eps_model: nn.Module,\n",
    "        betas: Tuple[float, float],\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ) -> None:\n",
    "        super(DDPM, self).__init__()\n",
    "        self.eps_model = eps_model\n",
    "\n",
    "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.\n",
    "        This implements Algorithm 1 in the paper.\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
    "            x.device\n",
    "        )  # t ~ Uniform(0, n_T)\n",
    "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * eps\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        return self.criterion(eps, self.eps_model(x_t, _ts / self.n_T))\n",
    "    \n",
    "    def forward_trajectory(self, x: torch.Tensor, device):\n",
    "        traj = [x]\n",
    "        for i in range(0, self.n_T):\n",
    "            b_t = 1 - self.alpha_t[i].to(device)\n",
    "            z = torch.randn(*x.shape).to(device)\n",
    "            x_t = torch.sqrt(1 - b_t) * traj[-1] + torch.sqrt(b_t) * z\n",
    "            traj.append(x_t)\n",
    "        return traj\n",
    "\n",
    "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
    "\n",
    "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "            eps = self.eps_model(x_i, i / self.n_T)\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "\n",
    "        return x_i\n",
    "\n",
    "    def replacement_sample(self, n_sample: int, size, device, motif_forward, motif_mask):\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
    "\n",
    "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "            motif_i = motif_forward[i]\n",
    "            x_i = x_i * (1 - motif_mask) + motif_i\n",
    "            eps = self.eps_model(x_i, i / self.n_T)\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "\n",
    "        return x_i\n",
    "    \n",
    "    def reverse_distribution(self, x_t, e_t, t):\n",
    "        a_t = self.alpha_t[t]\n",
    "        b_t = 1 - a_t\n",
    "        cum_a_t = self.alphabar_t[t]\n",
    "        pred_noise = (1 - a_t) / torch.sqrt(1 - cum_a_t) * e_t\n",
    "        mu_t_1 = 1 / torch.sqrt(a_t) * (x_t - pred_noise)\n",
    "        sd_t_1 = torch.sqrt(b_t)\n",
    "        return mu_t_1, sd_t_1\n",
    "    \n",
    "    def log_impt_weights(self, x_t, e_t, t, motif_forward_diffusion, motif_mask):\n",
    "        \"\"\"computes log importance weights\n",
    "        \"\"\"\n",
    "        if t ==0: return torch.zeros(x_t.shape[0])\n",
    "        mu, sd = self.reverse_distribution(x_t, e_t, t)\n",
    "        mu_M = mu * motif_mask\n",
    "        x_t_1_m = motif_forward_diffusion[t-1]\n",
    "\n",
    "        # compute un-normalized weighting factor for importance resampling step\n",
    "        log_w = -(1./2)*(x_t_1_m-mu_M)**2/(sd**2)\n",
    "        log_w = torch.sum(log_w, axis=[1, 2, 3])\n",
    "        log_w -= torch.logsumexp(log_w, 0)\n",
    "        return log_w\n",
    "\n",
    "    def smc_sample(\n",
    "            self,\n",
    "            n_sample: int,\n",
    "            size,\n",
    "            device,\n",
    "            motif_traj,\n",
    "            motif_mask,\n",
    "        ):\n",
    "    \n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
    "        weights = np.ones([n_sample])\n",
    "\n",
    "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "            motif_i = motif_traj[i]\n",
    "            x_i = x_i * (1 - motif_mask) + motif_i\n",
    "            eps = self.eps_model(x_i, i / self.n_T).to(device)\n",
    "\n",
    "            # compute importance weights\n",
    "            log_w = self.log_impt_weights(\n",
    "                x_i, eps, i, motif_traj, motif_mask)\n",
    "\n",
    "            # Update Self-normalized importance weights\n",
    "            weights = weights*torch.exp(log_w).cpu().detach().numpy()\n",
    "            weights /= np.sum(weights) # Re-normalize\n",
    "            \n",
    "            # Residual resample, but only if\n",
    "            #   (1) weights are sufficiently non-uniform, and\n",
    "            #   (2)(optionally) not too close to end of the trajectory\n",
    "            departure_from_uniform = np.sum(abs(n_sample*weights-1))\n",
    "            # if (departure_from_uniform > 0.75*n_sample) and t > self.n_T//10:\n",
    "            if departure_from_uniform > 0.75*n_sample:\n",
    "            \n",
    "                # print(t, \"resampling, departure=%0.02f\"%departure_from_uniform)\n",
    "                idcs, N_replace_t = residual_resample(weights)\n",
    "                # resample_times.append(t)\n",
    "                # N_replace.append(N_replace_t)\n",
    "\n",
    "                # Apply resampling\n",
    "                x_i, eps = x_i[idcs].to(device), eps[idcs].to(device)\n",
    "\n",
    "                # Reset weights to uniform\n",
    "                weights = np.ones_like(weights)/n_sample\n",
    "            \n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "\n",
    "        return x_i\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#         sampled_diffusion = [x_T]\n",
    "#         for t in reversed(range(T)):\n",
    "#             if t % log_freq == (log_freq-1): print(f'On {t}')\n",
    "\n",
    "#             # predict error with diffusion model\n",
    "#             x_t = sampled_diffusion[-1]\n",
    "#             e_t = inpaint.next_step_pred_with_motif(\n",
    "#                 x_t, t, exp, motif_forward_diffusion,\n",
    "#                 motif_idcs, inference_feats)\n",
    "\n",
    "#             # compute importance weights\n",
    "#             log_w = log_impt_weights(\n",
    "#                 prot_diffuser, x_t, e_t, t, motif_forward_diffusion,\n",
    "#                 motif_idcs)\n",
    "\n",
    "#             # Update Self-normalized importance weights\n",
    "#             weights = weights*torch.exp(log_w).cpu().detach().numpy()\n",
    "#             weights /= sum(weights) # Re-normalize\n",
    "#             ws.append(weights)\n",
    "\n",
    "#             # Residual resample, but only if\n",
    "#             #   (1) weights are sufficiently non-uniform, and\n",
    "#             #   (2)(optionally) not too close to end of the trajectory\n",
    "#             departure_from_uniform = np.sum(abs(N_particles*weights-1))\n",
    "#             #if (departure_from_uniform > 0.75*N_particles) and t > T//20: ()\n",
    "#             if departure_from_uniform > 0.75*N_particles:\n",
    "#                 print(t, \"resampling, departure=%0.02f\"%departure_from_uniform)\n",
    "#                 idcs, N_replace_t = residual_resample(weights)\n",
    "#                 resample_times.append(t)\n",
    "#                 N_replace.append(N_replace_t)\n",
    "\n",
    "#                 # Apply resampling\n",
    "#                 x_t, e_t = x_t[idcs], e_t[idcs]\n",
    "\n",
    "#                 # Reset weights to uniform\n",
    "#                 weights = np.ones_like(weights)/N_particles\n",
    "\n",
    "#             x_t_1 = prot_diffuser.ar_reverse_diffusion(x_t, e_t, t).type(torch.float32)\n",
    "#             x_t_1 *= inference_feats['bb_mask'][..., None].cpu()\n",
    "#             sampled_diffusion.append(x_t_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b01ec6-6f98-4488-988c-cf14e6886725",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "device = 'cuda:6'\n",
    "model_path = \"/data/rsg/chemistry/jyim/projects/protein_diffusion/notebooks/ddpm_mnist.pth\"\n",
    "\n",
    "ddpm = DDPM(eps_model=DummyEpsModel(1), betas=(1e-4, 0.02), n_T=1000)\n",
    "ddpm.to(device)\n",
    "\n",
    "tf = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))]\n",
    ")\n",
    "\n",
    "dataset = MNIST(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=tf,\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=20)\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7af5317-0b5b-40a9-8806-3dc7a9b2cb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_weights = torch.load(model_path)\n",
    "ddpm.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b545191c-f805-4e09-b57c-04f52e5edc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# for i in range(n_epoch):\n",
    "#     ddpm.train()\n",
    "\n",
    "#     pbar = tqdm(dataloader)\n",
    "#     for x, _ in pbar:\n",
    "#         optim.zero_grad()\n",
    "#         x = x.to(device)\n",
    "#         loss = ddpm(x)\n",
    "#         loss.backward()\n",
    "#         loss_ema = loss.item()\n",
    "#         pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "#         optim.step()\n",
    "\n",
    "#     ddpm.eval()\n",
    "#     with torch.no_grad():\n",
    "#         xh = ddpm.sample(16, (1, 28, 28), device)\n",
    "#         grid = make_grid(xh, nrow=4)\n",
    "#         save_image(grid, f\"/data/rsg/chemistry/jyim/projects/protein_diffusion/notebooks/contents/ddpm_sample_{i}.png\")\n",
    "\n",
    "#         # save model\n",
    "#         torch.save(ddpm.state_dict(), \"/data/rsg/chemistry/jyim/projects/protein_diffusion/notebooks/ddpm_mnist.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c871ef8-1ed9-4eee-ae0b-174b4c48c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, _) in enumerate(dataloader):\n",
    "    mnist_batch = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c73aa2f4-91aa-4710-b1c9-2ac94003c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_base_dir = '/data/rsg/chemistry/jyim/projects/protein_diffusion/notebooks/contents/inpainting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dcc0bf49-353e-4a7b-a741-2f800a0c2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24278/1003193641.py:5: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"cmap\" which is no longer supported as of 3.3 and will become an error in 3.6\n",
      "  plt.savefig(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 14\n",
    "conditioning = torch.clone(mnist_batch[idx:(idx+1)]).to(device)\n",
    "plt.imshow(conditioning[0, 0].cpu().numpy(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.savefig(\n",
    "    os.path.join(save_base_dir, 'full.png'),\n",
    "    cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "788fe311-90b5-49ea-b3cf-5bbad3d50389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24278/2992995488.py:7: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"cmap\" which is no longer supported as of 3.3 and will become an error in 3.6\n",
      "  plt.savefig(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_pixels = conditioning.shape[2]\n",
    "conditioning_mask = torch.zeros_like(conditioning).to(device)\n",
    "conditioning_mask[:, :,  :, :(num_pixels//2)] = 1\n",
    "conditioning *= conditioning_mask\n",
    "plt.imshow(conditioning[0, 0].cpu().numpy(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.savefig(\n",
    "    os.path.join(save_base_dir, 'condition.png'),\n",
    "    cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77dd5be5-1192-4b85-b04a-83797239be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forward diffusion\n",
    "motif_traj = ddpm.forward_trajectory(conditioning, device)\n",
    "motif_traj = [conditioning_mask * x for x in motif_traj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36829aa5-5037-4fd0-b33a-576794940ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c7f5218b-038d-479d-85e6-914eebd47d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "batched_motif_traj = [x.repeat(batch_size, 1, 1, 1) for x in motif_traj]\n",
    "batched_conditioning_mask = conditioning_mask.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    xh = ddpm.replacement_sample(batch_size, (1, 28, 28), device, batched_motif_traj, batched_conditioning_mask)\n",
    "    grid = make_grid(xh, nrow=int(np.sqrt(batch_size)))\n",
    "    save_image(\n",
    "        grid,\n",
    "        os.path.join(save_base_dir, 'replacement.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f37f8325-dde9-44a5-9404-40e3f04a812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "particle_size = 16\n",
    "batched_motif_traj = [x.repeat(particle_size, 1, 1, 1) for x in motif_traj]\n",
    "batched_conditioning_mask = conditioning_mask.repeat(particle_size, 1, 1, 1)\n",
    "\n",
    "batch_size = 16\n",
    "smc_samples = []\n",
    "ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "        path_name = os.path.join(save_base_dir, f'smc_k_{particle_size}_{i}.png')\n",
    "        xh = ddpm.smc_sample(particle_size, (1, 28, 28), device, batched_motif_traj, batched_conditioning_mask)\n",
    "        smc_samples.append(xh)\n",
    "        grid = make_grid(xh, nrow=int(np.sqrt(particle_size)))\n",
    "        save_image(grid, path_name)\n",
    "        print(i)\n",
    "new_smc_batch = torch.stack([x[0] for x in smc_samples])\n",
    "grid = make_grid(new_smc_batch, nrow=int(np.sqrt(batch_size)))\n",
    "save_image(grid, os.path.join(save_base_dir, f'grid_smc_k_{particle_size}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7edc609f-bc90-4837-92eb-322c32117bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "particle_size = 64\n",
    "batched_motif_traj = [x.repeat(particle_size, 1, 1, 1) for x in motif_traj]\n",
    "batched_conditioning_mask = conditioning_mask.repeat(particle_size, 1, 1, 1)\n",
    "\n",
    "batch_size = 16\n",
    "smc_samples = []\n",
    "ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "        path_name = os.path.join(save_base_dir, f'smc_k_{particle_size}_{i}.png')\n",
    "        xh = ddpm.smc_sample(particle_size, (1, 28, 28), device, batched_motif_traj, batched_conditioning_mask)\n",
    "        smc_samples.append(xh)\n",
    "        grid = make_grid(xh, nrow=int(np.sqrt(particle_size)))\n",
    "        save_image(grid, path_name)\n",
    "        print(i)\n",
    "new_smc_batch = torch.stack([x[0] for x in smc_samples])\n",
    "grid = make_grid(new_smc_batch, nrow=int(np.sqrt(batch_size)))\n",
    "save_image(grid, os.path.join(save_base_dir, f'grid_smc_k_{particle_size}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8b11810-c446-4210-9784-db293599ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd9f3bc0-f640-4cf0-b71d-e285d5263ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc53bef-3073-4967-8f61-7ebb0b981a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af01e96-d796-41a1-a3da-04d24d44818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(smc_samples[3], nrow=4)\n",
    "save_image(grid, f\"/data/rsg/chemistry/jyim/projects/protein_diffusion/notebooks/contents/smc_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955d018-d93e-49f6-aa44-262365f04ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7759edc-1a3e-4f64-b90b-65e974478085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89abc08-1b65-4eee-9409-d3432655a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_diffusion = [x_T]\n",
    "for t in reversed(range(num_t)):\n",
    "    x_t = sampled_diffusion[-1]\n",
    "    e_t = next_step_pred_with_motif(\n",
    "        x_t, t, exp, motif_forward_diffusion,\n",
    "        motif_idcs, inference_feats)\n",
    "\n",
    "    x_t_1 = prot_diffuser.ar_reverse_diffusion(x_t, e_t, t).type(torch.float32)\n",
    "    x_t_1 *= inference_feats['bb_mask'][..., None].cpu()\n",
    "\n",
    "    sampled_diffusion.append(x_t_1)\n",
    "    if t % log_freq == (log_freq-1):\n",
    "        print(f'On {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9d777-8690-412a-86a2-1d8a8c14ab14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ae174-d536-462a-b122-df608638c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac7c8d-1dd0-4762-9c8b-78a98f0f37c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67fb9f-0587-4b8f-a46e-806c40414418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1d2e8-eb84-4655-9334-b948cdc94fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from inpainting import inpaint\n",
    "\n",
    "def inpaint_particle_filter(\n",
    "    N_particles, exp, target_len, motif_forward_diffusion,\n",
    "    motif_idcs, prot_diffuser, log_freq = 100):\n",
    "    T = exp.cfg.experiment.T\n",
    "\n",
    "    # characteristics of particle filtering trajectory & resampling\n",
    "    ws, N_replace, resample_times = [], [], []\n",
    "\n",
    "    # set mask to have correct batch dimension\n",
    "    inference_feats= {\n",
    "            'bb_mask':torch.zeros([N_particles, target_len]),\n",
    "            'residue_index':torch.tile(torch.arange(target_len),[N_particles,1])\n",
    "            }\n",
    "    inference_feats['bb_mask'][:, :target_len] = 1.\n",
    "\n",
    "    # initialize weights to ones.\n",
    "    weights = np.ones([N_particles])\n",
    "\n",
    "    # Sample x_T as random noise\n",
    "    x_T = torch.Tensor(np.random.normal(\n",
    "        size=list(inference_feats['bb_mask'].shape) + [3])).type(torch.float32)\n",
    "\n",
    "    sampled_diffusion = [x_T]\n",
    "    for t in reversed(range(T)):\n",
    "        if t % log_freq == (log_freq-1): print(f'On {t}')\n",
    "\n",
    "        # predict error with diffusion model\n",
    "        x_t = sampled_diffusion[-1]\n",
    "        e_t = inpaint.next_step_pred_with_motif(\n",
    "            x_t, t, exp, motif_forward_diffusion,\n",
    "            motif_idcs, inference_feats)\n",
    "\n",
    "        # compute importance weights\n",
    "        log_w = log_impt_weights(\n",
    "            prot_diffuser, x_t, e_t, t, motif_forward_diffusion,\n",
    "            motif_idcs)\n",
    "\n",
    "        # Update Self-normalized importance weights\n",
    "        weights = weights*torch.exp(log_w).cpu().detach().numpy()\n",
    "        weights /= sum(weights) # Re-normalize\n",
    "        ws.append(weights)\n",
    "\n",
    "        # Residual resample, but only if\n",
    "        #   (1) weights are sufficiently non-uniform, and\n",
    "        #   (2)(optionally) not too close to end of the trajectory\n",
    "        departure_from_uniform = np.sum(abs(N_particles*weights-1))\n",
    "        #if (departure_from_uniform > 0.75*N_particles) and t > T//20: ()\n",
    "        if departure_from_uniform > 0.75*N_particles:\n",
    "            print(t, \"resampling, departure=%0.02f\"%departure_from_uniform)\n",
    "            idcs, N_replace_t = residual_resample(weights)\n",
    "            resample_times.append(t)\n",
    "            N_replace.append(N_replace_t)\n",
    "\n",
    "            # Apply resampling\n",
    "            x_t, e_t = x_t[idcs], e_t[idcs]\n",
    "\n",
    "            # Reset weights to uniform\n",
    "            weights = np.ones_like(weights)/N_particles\n",
    "\n",
    "        x_t_1 = prot_diffuser.ar_reverse_diffusion(x_t, e_t, t).type(torch.float32)\n",
    "        x_t_1 *= inference_feats['bb_mask'][..., None].cpu()\n",
    "        sampled_diffusion.append(x_t_1)\n",
    "\n",
    "    return sampled_diffusion, N_replace, ws, resample_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53812956-e3a2-4850-a633-de3e53ebd4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57506e-b223-4e17-a77e-4de2613f0ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2281505-954c-4556-9c8d-2249e55e9fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168c925-d3eb-442e-bccf-f03599097734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9421c-74a7-4f34-a3c3-464052c53835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e23f9-82c4-473c-a6a8-f32f609e0634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
