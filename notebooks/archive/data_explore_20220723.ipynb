{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from dateutil import parser\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "import os\n",
    "from collections import namedtuple, defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "from data import chemical\n",
    "from data import residue_constants\n",
    "from data import utils as du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DistilledDataset(pdb_IDs, loader_pdb, loader_pdb_fixbb, pdb_dict,\n",
    "                             compl_IDs, loader_complex, loader_complex_fixbb, compl_dict,\n",
    "                             neg_IDs, loader_complex, neg_dict,\n",
    "                             fb_IDs, loader_fb, loader_fb_fixbb, fb_dict,\n",
    "                             homo, self.loader_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = DistributedWeightedSampler(train_set, pdb_weights, compl_weights, neg_weights, fb_weights, p_seq2str,\n",
    "                                           num_example_per_epoch=N_EXAMPLE_PER_EPOCH,\n",
    "                                           num_replicas=world_size, rank=rank, fraction_fb=0.5, fraction_compl=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_set, sampler=train_sampler, batch_size=self.batch_size, **LOAD_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_loader_params(args):\n",
    "    PARAMS = {\n",
    "        \"COMPL_LIST\" : \"%s/list.hetero.csv\"%compl_dir,\n",
    "        \"HOMO_LIST\" : \"%s/list.homo.csv\"%compl_dir,\n",
    "        \"NEGATIVE_LIST\" : \"%s/list.negative.csv\"%compl_dir,\n",
    "        \"PDB_LIST\"   : \"%s/list_v02.csv\"%base_dir,\n",
    "        #\"PDB_LIST\"    : \"/gscratch2/list_2021AUG02.csv\",\n",
    "        \"FB_LIST\"    : \"%s/list_b1-3.csv\"%fb_dir,\n",
    "        \"VAL_PDB\"    : \"%s/val/xaa\"%base_dir,\n",
    "        #\"VAL_PDB\"   : \"/gscratch2/PDB_val/xaa\",\n",
    "        \"VAL_COMPL\"  : \"%s/val_lists/xaa\"%compl_dir,\n",
    "        \"VAL_NEG\"    : \"%s/val_lists/xaa.neg\"%compl_dir,\n",
    "        \"DATAPKL\"    : \"./dataset.pkl\", # cache for faster loading\n",
    "        \"PDB_DIR\"    : base_dir,\n",
    "        \"FB_DIR\"     : fb_dir,\n",
    "        \"COMPL_DIR\"  : compl_dir,\n",
    "        \"MINTPLT\" : 0,\n",
    "        \"MAXTPLT\" : 5,\n",
    "        \"MINSEQ\"  : 1,\n",
    "        \"MAXSEQ\"  : 1024,\n",
    "        \"MAXLAT\"  : 128, \n",
    "        \"CROP\"    : 256,\n",
    "        \"DATCUT\"  : \"2020-Apr-30\",\n",
    "        \"RESCUT\"  : 5.0,\n",
    "        \"BLOCKCUT\": 5,\n",
    "        \"PLDDTCUT\": 70.0,\n",
    "        \"SCCUT\"   : 90.0,\n",
    "        \"ROWS\"    : 1,\n",
    "        \"SEQID\"   : 95.0,\n",
    "        \"MAXCYCLE\": 4,\n",
    "        \"HAL_MASK_HIGH\": 35, #added by JW for hal masking\n",
    "        \"HAL_MASK_LOW\": 10,\n",
    "        \"HAL_MASK_HIGH_AR\": 50,\n",
    "        \"HAL_MASK_LOW_AR\": 20,\n",
    "        \"COMPLEX_HAL_MASK_HIGH\": 35,\n",
    "        \"COMPLEX_HAL_MASK_LOW\": 10,\n",
    "        \"COMPLEX_HAL_MASK_HIGH_AR\": 50,\n",
    "        \"COMPLEX_HAL_MASK_LOW_AR\": 20,\n",
    "        \"FLANK_HIGH\": 6,\n",
    "        \"FLANK_LOW\" : 3,\n",
    "        \"STR2SEQ_FULL_LOW\" : 0.9,\n",
    "        \"STR2SEQ_FULL_HIGH\" : 1.0,\n",
    "        \"MAX_LENGTH\" : 260,\n",
    "        \"MAX_COMPLEX_CHAIN\" : 200,\n",
    "        \"TASK_NAMES\" : ['seq2str'],\n",
    "        \"TASK_P\" : [1.0],\n",
    "        \"DIFF_MASK_LOW\":args.diff_mask_low,\n",
    "        \"DIFF_MASK_HIGH\":args.diff_mask_high,\n",
    "\n",
    "    }\n",
    "    for param in PARAMS:\n",
    "        if hasattr(args, param.lower()):\n",
    "            PARAMS[param] = getattr(args, param.lower())\n",
    "\n",
    "    print('This is params from get train valid')\n",
    "    for key,val in PARAMS.items():\n",
    "        print(key, val)\n",
    "    return PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_set(params, OFFSET=1000000):\n",
    "\n",
    "    if (not os.path.exists(params['DATAPKL'])):\n",
    "        # read validation IDs for PDB set\n",
    "        val_pdb_ids = set([int(l) for l in open(params['VAL_PDB']).readlines()])\n",
    "        val_compl_ids = set([int(l) for l in open(params['VAL_COMPL']).readlines()])\n",
    "        val_neg_ids = set([int(l)+OFFSET for l in open(params['VAL_NEG']).readlines()])\n",
    "\n",
    "\n",
    "        # read validation IDs for PDB set\n",
    "        val_pdb_ids = set([int(l) for l in open(params['VAL_PDB']).readlines()])\n",
    "        val_compl_ids = set([int(l) for l in open(params['VAL_COMPL']).readlines()])\n",
    "        val_neg_ids = set([int(l)+OFFSET for l in open(params['VAL_NEG']).readlines()])\n",
    "    \n",
    "        # read homo-oligomer list\n",
    "        homo = {}\n",
    "        # with open(params['HOMO_LIST'], 'r') as f:\n",
    "        #     reader = csv.reader(f)\n",
    "        #     next(reader)\n",
    "        #     # read pdbA, pdbB, bioA, opA, bioB, opB\n",
    "        #     rows = [[r[0], r[1], int(r[2]), int(r[3]), int(r[4]), int(r[5])] for r in reader]\n",
    "        # for r in rows:\n",
    "        #     if r[0] in homo.keys():\n",
    "        #         homo[r[0]].append(r[1:])\n",
    "        #     else:\n",
    "        #         homo[r[0]] = [r[1:]]\n",
    "\n",
    "        # read & clean list.csv\n",
    "        with open(params['PDB_LIST'], 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            rows = [[r[0],r[3],int(r[4]), int(r[-1].strip())] for r in reader\n",
    "                    if float(r[2])<=params['RESCUT'] and\n",
    "                    parser.parse(r[1])<=parser.parse(params['DATCUT']) and len(r[-2]) <= params['MAX_LENGTH'] and len(r[-2]) >= 60] #added length max so only have full chains, and minimum length of 60aa\n",
    "\n",
    "        # compile training and validation sets\n",
    "        val_hash = list()\n",
    "        train_pdb = {}\n",
    "        valid_pdb = {}\n",
    "        valid_homo = {}\n",
    "        for r in rows:\n",
    "            if r[2] in val_pdb_ids:\n",
    "                val_hash.append(r[1])\n",
    "                if r[2] in valid_pdb.keys():\n",
    "                    valid_pdb[r[2]].append((r[:2], r[-1]))\n",
    "                else:\n",
    "                    valid_pdb[r[2]] = [(r[:2], r[-1])]\n",
    "                #\n",
    "                if r[0] in homo:\n",
    "                    if r[2] in valid_homo.keys():\n",
    "                        valid_homo[r[2]].append((r[:2], r[-1]))\n",
    "                    else:\n",
    "                        valid_homo[r[2]] = [(r[:2], r[-1])]\n",
    "            else:\n",
    "                if r[2] in train_pdb.keys():\n",
    "                    train_pdb[r[2]].append((r[:2], r[-1]))\n",
    "                else:\n",
    "                    train_pdb[r[2]] = [(r[:2], r[-1])]\n",
    "        val_hash = set(val_hash)\n",
    "        \n",
    "        # compile facebook model sets\n",
    "        with open(params['FB_LIST'], 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            rows = [[r[0],r[2],int(r[3]),len(r[-1].strip())] for r in reader\n",
    "                     if float(r[1]) > 80.0 and\n",
    "                     len(r[-1].strip()) > 100 and len(r[-1].strip()) <= params['MAX_LENGTH']] #added max length to allow only full chains. Also reduced minimum length to 100aa\n",
    "        \n",
    "        fb = {}\n",
    "        \n",
    "        for r in rows:\n",
    "            if r[2] in fb.keys():\n",
    "                fb[r[2]].append((r[:2], r[-1]))\n",
    "            else:\n",
    "                fb[r[2]] = [(r[:2], r[-1])]\n",
    "        \n",
    "        #compile complex sets\n",
    "        with open(params['COMPL_LIST'], 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            # read complex_pdb, pMSA_hash, complex_cluster, length, taxID, assembly (bioA,opA,bioB,opB)\n",
    "            rows = [[r[0], r[3], int(r[4]), [int(plen) for plen in r[5].split(':')], r[6] , [int(r[7]), int(r[8]), int(r[9]), int(r[10])]] for r in reader\n",
    "                     if float(r[2]) <= params['RESCUT'] and\n",
    "                     parser.parse(r[1]) <= parser.parse(params['DATCUT']) and min([int(i) for i in r[5].split(\":\")]) < params['MAX_COMPLEX_CHAIN'] and min([int(i) for i in r[5].split(\":\")]) > 50] #require one chain of the hetero complexes to be smaller than a certain value so it can be kept complete. This chain must also be > 50aa long.\n",
    "\n",
    "        train_compl = {}\n",
    "        valid_compl = {}\n",
    "        for r in rows:\n",
    "            if r[2] in val_compl_ids:\n",
    "                if r[2] in valid_compl.keys():\n",
    "                    valid_compl[r[2]].append((r[:2], r[-3], r[-2], r[-1])) # ((pdb, hash), length, taxID, assembly, negative?)\n",
    "                else:\n",
    "                    valid_compl[r[2]] = [(r[:2], r[-3], r[-2], r[-1])]\n",
    "            else:\n",
    "                # if subunits are included in PDB validation set, exclude them from training\n",
    "                hashA, hashB = r[1].split('_')\n",
    "                if hashA in val_hash:\n",
    "                    continue\n",
    "                if hashB in val_hash:\n",
    "                    continue\n",
    "                if r[2] in train_compl.keys():\n",
    "                    train_compl[r[2]].append((r[:2], r[-3], r[-2], r[-1]))\n",
    "                else:\n",
    "                    train_compl[r[2]] = [(r[:2], r[-3], r[-2], r[-1])]\n",
    "\n",
    "        # compile negative examples\n",
    "        # remove pairs if any of the subunits are included in validation set\n",
    "        # with open(params['NEGATIVE_LIST'], 'r') as f:\n",
    "        #     reader = csv.reader(f)\n",
    "        #     next(reader)\n",
    "        #     # read complex_pdb, pMSA_hash, complex_cluster, length, taxonomy\n",
    "        #     rows = [[r[0],r[3],OFFSET+int(r[4]),[int(plen) for plen in r[5].split(':')],r[6]] for r in reader\n",
    "        #             if float(r[2])<=params['RESCUT'] and\n",
    "        #             parser.parse(r[1])<=parser.parse(params['DATCUT'])]\n",
    "\n",
    "        train_neg = {}\n",
    "        valid_neg = {}\n",
    "        # for r in rows:\n",
    "        #     if r[2] in val_neg_ids:\n",
    "        #         if r[2] in valid_neg.keys():\n",
    "        #             valid_neg[r[2]].append((r[:2], r[-2], r[-1], []))\n",
    "        #         else:\n",
    "        #             valid_neg[r[2]] = [(r[:2], r[-2], r[-1], [])]\n",
    "        #     else:\n",
    "        #         hashA, hashB = r[1].split('_')\n",
    "        #         if hashA in val_hash:\n",
    "        #             continue\n",
    "        #         if hashB in val_hash:\n",
    "        #             continue\n",
    "        #         if r[2] in train_neg.keys():\n",
    "        #             train_neg[r[2]].append((r[:2], r[-2], r[-1], []))\n",
    "        #         else:\n",
    "        #             train_neg[r[2]] = [(r[:2], r[-2], r[-1], [])]\n",
    "    \n",
    "        # Get average chain length in each cluster and calculate weights\n",
    "        pdb_IDs = list(train_pdb.keys())\n",
    "        fb_IDs = list(fb.keys())\n",
    "        compl_IDs = list(train_compl.keys())\n",
    "        neg_IDs = list(train_neg.keys())\n",
    "\n",
    "        pdb_weights = list()\n",
    "        fb_weights = list()\n",
    "        compl_weights = list()\n",
    "        neg_weights = list()\n",
    "        for key in pdb_IDs:\n",
    "            plen = sum([plen for _, plen in train_pdb[key]]) // len(train_pdb[key])\n",
    "            w = (1/512.)*max(min(float(plen),512.),256.)\n",
    "            pdb_weights.append(w)\n",
    "    \n",
    "        for key in fb_IDs:\n",
    "            plen = sum([plen for _, plen in fb[key]]) // len(fb[key])\n",
    "            w = (1/512.)*max(min(float(plen),512.),256.)\n",
    "            fb_weights.append(w)\n",
    "    \n",
    "        for key in compl_IDs:\n",
    "            plen = sum([sum(plen) for _, plen, _, _ in train_compl[key]]) // len(train_compl[key])\n",
    "            w = (1/512.)*max(min(float(plen),512.),256.)\n",
    "            compl_weights.append(w)\n",
    "    \n",
    "        for key in neg_IDs:\n",
    "            plen = sum([sum(plen) for _, plen, _, _ in train_neg[key]]) // len(train_neg[key])\n",
    "            w = (1/512.)*max(min(float(plen),512.),256.)\n",
    "            neg_weights.append(w)\n",
    "        # save\n",
    "        obj = (\n",
    "           pdb_IDs, pdb_weights, train_pdb,\n",
    "           fb_IDs, fb_weights, fb,\n",
    "           compl_IDs, compl_weights, train_compl,\n",
    "           neg_IDs, neg_weights, train_neg,\n",
    "           valid_pdb, valid_homo, valid_compl, valid_neg, homo\n",
    "        )\n",
    "        with open(params[\"DATAPKL\"], \"wb\") as f:\n",
    "            print ('Writing',params[\"DATAPKL\"])\n",
    "            pickle.dump(obj, f)\n",
    "            print ('Done')\n",
    "\n",
    "    else:\n",
    "        with open(params[\"DATAPKL\"], \"rb\") as f:\n",
    "            print ('Loading',params[\"DATAPKL\"])\n",
    "            (\n",
    "               pdb_IDs, pdb_weights, train_pdb,\n",
    "               fb_IDs, fb_weights, fb,\n",
    "               compl_IDs, compl_weights, train_compl,\n",
    "               neg_IDs, neg_weights, train_neg,\n",
    "               valid_pdb, valid_homo, valid_compl, valid_neg, homo\n",
    "            ) = pickle.load(f)\n",
    "            print ('Done')\n",
    "\n",
    "    return (pdb_IDs, torch.tensor(pdb_weights).float(), train_pdb), \\\n",
    "           (fb_IDs, torch.tensor(fb_weights).float(), fb), \\\n",
    "           (compl_IDs, torch.tensor(compl_weights).float(), train_compl), \\\n",
    "           (neg_IDs, torch.tensor(neg_weights).float(), train_neg),\\\n",
    "           valid_pdb, valid_homo, valid_compl, valid_neg, homo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/projects/ml/TrRosetta/PDB-2021AUG02\"\n",
    "compl_dir = \"/projects/ml/RoseTTAComplex\"\n",
    "fb_dir = \"/projects/ml/TrRosetta/fb_af\"\n",
    "if not os.path.exists(base_dir):\n",
    "    # training on AWS\n",
    "    base_dir = \"/data/databases/PDB-2021AUG02\"\n",
    "    fb_dir = \"/data/databases/fb_af\"\n",
    "    compl_dir = \"/data/databases/RoseTTAComplex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DistilledDataset(pdb_IDs, loader_pdb, loader_pdb_fixbb, pdb_dict,\n",
    "                             compl_IDs, loader_complex, loader_complex_fixbb, compl_dict,\n",
    "                             neg_IDs, loader_complex, neg_dict,\n",
    "                             fb_IDs, loader_fb, loader_fb_fixbb, fb_dict,\n",
    "                             homo, self.loader_param)\n",
    "\n",
    "valid_pdb_set = Dataset(list(valid_pdb.keys())[:self.n_valid_pdb],\n",
    "                        loader_pdb, valid_pdb,\n",
    "                        self.loader_param, homo, p_homo_cut=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilledDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 pdb_IDs,\n",
    "                 pdb_loader,\n",
    "                 pdb_loader_fixbb,\n",
    "                 pdb_dict,\n",
    "                 compl_IDs,\n",
    "                 compl_loader,\n",
    "                 compl_loader_fixbb,\n",
    "                 compl_dict,\n",
    "                 neg_IDs,\n",
    "                 neg_loader,\n",
    "                 neg_dict,\n",
    "                 fb_IDs,\n",
    "                 fb_loader,\n",
    "                 fb_loader_fixbb,\n",
    "                 fb_dict,\n",
    "                 homo,\n",
    "                 params,\n",
    "                 p_homo_cut=0.5):\n",
    "        #\n",
    "        self.pdb_IDs     = pdb_IDs\n",
    "        self.pdb_dict    = pdb_dict\n",
    "        self.pdb_loaders = {'seq2str':      pdb_loader,\n",
    "                            'str2seq':      pdb_loader_fixbb, \n",
    "                            'str2seq_full': pdb_loader_fixbb, \n",
    "                            'hal':          pdb_loader_fixbb, \n",
    "                            'hal_ar':       pdb_loader_fixbb,\n",
    "                            'diff':         pdb_loader_fixbb}\n",
    "\n",
    "\n",
    "        self.compl_IDs     = compl_IDs\n",
    "        self.compl_dict    = compl_dict\n",
    "        self.compl_loaders = {'seq2str':     compl_loader,\n",
    "                              'str2seq':     compl_loader_fixbb, \n",
    "                              'str2seq_full':compl_loader_fixbb, \n",
    "                              'hal':         compl_loader_fixbb, \n",
    "                              'hal_ar':      compl_loader_fixbb,\n",
    "                              'diff':        compl_loader_fixbb}\n",
    "\n",
    "\n",
    "        self.neg_IDs    = neg_IDs\n",
    "        self.neg_loader = neg_loader\n",
    "        self.neg_dict   = neg_dict\n",
    "\n",
    "\n",
    "        self.fb_IDs     = fb_IDs\n",
    "        self.fb_dict    = fb_dict\n",
    "        self.fb_loaders = { 'seq2str':      fb_loader,\n",
    "                            'str2seq':      fb_loader_fixbb, \n",
    "                            'str2seq_full': fb_loader_fixbb, \n",
    "                            'hal':          fb_loader_fixbb, \n",
    "                            'hal_ar':       fb_loader_fixbb,\n",
    "                            'diff':         fb_loader_fixbb}\n",
    "\n",
    "        self.homo = homo\n",
    "        self.params = params\n",
    "        self.p_task = params['TASK_P']\n",
    "        self.task_names = params['TASK_NAMES']\n",
    "        self.unclamp_cut = 0.9\n",
    "        self.p_homo_cut = p_homo_cut\n",
    "        \n",
    "        self.compl_inds = np.arange(len(self.compl_IDs))\n",
    "        self.neg_inds = np.arange(len(self.neg_IDs))\n",
    "        self.fb_inds = np.arange(len(self.fb_IDs))\n",
    "        self.pdb_inds = np.arange(len(self.pdb_IDs))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fb_inds) + len(self.pdb_inds) + len(self.compl_inds) + len(self.neg_inds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        p_unclamp = np.random.rand()\n",
    "\n",
    "        #choose task if not from negative set (which is always seq2str)\n",
    "        if index < len(self.fb_inds) + len(self.pdb_inds) + len(self.compl_inds):\n",
    "            task_idx = np.random.choice(np.arange(len(self.task_names)), 1, p=self.p_task)[0]\n",
    "            task = self.task_names[task_idx]\n",
    "        else:\n",
    "            task = 'seq2str'\n",
    "\n",
    "        if index >= len(self.fb_inds) + len(self.pdb_inds) + len(self.compl_inds): # from negative set\n",
    "            # print('Chose negative')\n",
    "            chosen_dataset='negative'\n",
    "            ID = self.neg_IDs[index-len(self.fb_inds)-len(self.pdb_inds)-len(self.compl_inds)]\n",
    "            sel_idx = np.random.randint(0, len(self.neg_dict[ID]))\n",
    "            out = self.neg_loader(self.neg_dict[ID][sel_idx][0], self.neg_dict[ID][sel_idx][1], self.neg_dict[ID][sel_idx][2], self.neg_dict[ID][sel_idx][3], self.params, negative=True)\n",
    "\n",
    "        elif index >= len(self.fb_inds) + len(self.pdb_inds): # from complex set\n",
    "            chosen_dataset='complex'\n",
    "            # print('Chose complex')\n",
    "            ID = self.compl_IDs[index-len(self.fb_inds)-len(self.pdb_inds)]\n",
    "            sel_idx = np.random.randint(0, len(self.compl_dict[ID]))\n",
    "            if self.compl_dict[ID][sel_idx][0][0] in self.homo: #homooligomer so do seq2str\n",
    "                task='seq2str'\n",
    "            chosen_loader = self.compl_loaders[task]\n",
    "            out = chosen_loader(self.compl_dict[ID][sel_idx][0], self.compl_dict[ID][sel_idx][1],self.compl_dict[ID][sel_idx][2], self.compl_dict[ID][sel_idx][3], self.params, negative=False)\n",
    "        \n",
    "        elif index >= len(self.fb_inds): # from PDB set\n",
    "            chosen_dataset='pdb'\n",
    "            # print('Chose pdb')\n",
    "            ID = self.pdb_IDs[index-len(self.fb_inds)]\n",
    "            sel_idx = np.random.randint(0, len(self.pdb_dict[ID]))\n",
    "            chosen_loader = self.pdb_loaders[task]\n",
    "            if p_unclamp > self.unclamp_cut:\n",
    "                out = chosen_loader(self.pdb_dict[ID][sel_idx][0], self.params, self.homo, unclamp=True, p_homo_cut=self.p_homo_cut)\n",
    "            else:\n",
    "                out = chosen_loader(self.pdb_dict[ID][sel_idx][0], self.params, self.homo, unclamp=False, p_homo_cut=self.p_homo_cut)\n",
    "        else: # from FB set\n",
    "            chosen_dataset='fb'\n",
    "            # print('Chose fb')\n",
    "            ID = self.fb_IDs[index]\n",
    "            sel_idx = np.random.randint(0, len(self.fb_dict[ID]))\n",
    "            chosen_loader = self.fb_loaders[task]\n",
    "            if p_unclamp > self.unclamp_cut:\n",
    "                out = chosen_loader(self.fb_dict[ID][sel_idx][0], self.params, unclamp=True)\n",
    "            else:\n",
    "                out = chosen_loader(self.fb_dict[ID][sel_idx][0], self.params, unclamp=False)\n",
    "\n",
    "        (seq, msa, msa_masked, msa_full, mask_msa, true_crds, atom_mask, idx_pdb, xyz_t, t1d, xyz_prev, same_chain, unclamp, negative, complete_chain, atom_mask) = out\n",
    "\n",
    "        # get masks for example\n",
    "        # Commenting this out. Because of popping Nans, need a new way of dealing with complexes, which will break the inpainting-style task mask generators.\n",
    "        \"\"\"\n",
    "        if complete_chain[0] == 0:\n",
    "            if complete_chain[1] is not None:\n",
    "                complete_chain = [0,complete_chain[1]-1] #first and last index of the first chain (complete_chain[1] is length of first chain)\n",
    "            else:\n",
    "                complete_chain = [0, same_chain.shape[0] -1] #first and last index of full chain\n",
    "        else:\n",
    "            complete_chain=[complete_chain[1],same_chain.shape[0]-1]\n",
    "        \"\"\"\n",
    "        if complete_chain[1] is not None:\n",
    "            chain_tensor, contacts = get_contacts(complete_chain, xyz_t)\n",
    "        else:\n",
    "            # make tensor of zeros to stable onto t1d for monomers (i.e. no contacts)\n",
    "            contacts = torch.zeros(xyz_t.shape[1])\n",
    "            \n",
    "        #### DJ/JW alteration: Pop any NaN residues from tensors for diffuion training \n",
    "        # print('Printing shapes')\n",
    "        # print(\"seq \",seq.shape)\n",
    "        # print(\"msa \",msa.shape)\n",
    "        # print(\"msa_masked \",msa_masked.shape)\n",
    "        # print(\"msa_full \",msa_full.shape)\n",
    "        # print(\"mask_msa \",mask_msa.shape)\n",
    "        # print(\"true_crds \",true_crds.shape)\n",
    "        # print(\"atom_mask \",atom_mask.shape )\n",
    "        # print(\"idx_pdb \",idx_pdb.shape)\n",
    "        # print(\"xyz_t \",xyz_t.shape)\n",
    "        # print(\"t1d \",t1d.shape)\n",
    "        # print(\"xyz_prev \",xyz_prev.shape)\n",
    "        # print(\"unclamp \",unclamp)\n",
    "        # print(\"atom_mask \",atom_mask.shape)\n",
    "        # print('same chain ',same_chain.shape)\n",
    "        pop   = (atom_mask[:,:3]).squeeze().any(dim=-1) # will be true if any of the backbone atoms were False in atom mask \n",
    "        N     = pop.sum()\n",
    "        pop2d = pop[None,:] * pop[:,None]\n",
    "\n",
    "        seq         = seq[:,pop]\n",
    "        msa         = msa[:,:,pop]\n",
    "        msa_masked  = msa_masked[:,:,pop]\n",
    "        msa_full    = msa_full[:,:,pop]\n",
    "        mask_msa    = mask_msa[:,:,pop]\n",
    "        true_crds   = true_crds[pop]\n",
    "        atom_mask   = atom_mask[pop]\n",
    "        idx_pdb     = idx_pdb[pop]\n",
    "        xyz_t       = xyz_t[:,pop]\n",
    "        t1d         = t1d[:,pop]\n",
    "        xyz_prev    = xyz_prev[pop]\n",
    "        same_chain  = same_chain[pop2d].reshape(N,N)\n",
    "        contacts = contacts[pop]\n",
    "\n",
    "        if complete_chain[1] is not None:\n",
    "            complete_chain = chain_tensor[pop]\n",
    "        \n",
    "        #Concatenate on the contacts tensor onto t1d\n",
    "        t1d = torch.cat((t1d, contacts[None,...,None]), dim=-1)\n",
    "        if chosen_dataset != 'complex':\n",
    "            assert torch.sum(t1d[:,:,-1]) == 0 \n",
    "        # print('Printing shapes after popping')\n",
    "        # print(\"seq \",seq.shape)\n",
    "        # print(\"msa \",msa.shape)\n",
    "        # print(\"msa_masked \",msa_masked.shape)\n",
    "        # print(\"msa_full \",msa_full.shape)\n",
    "        # print(\"mask_msa \",mask_msa.shape)\n",
    "        # print(\"true_crds \",true_crds.shape)\n",
    "        # print(\"atom_mask \",atom_mask.shape )\n",
    "        # print(\"idx_pdb \",idx_pdb.shape)\n",
    "        # print(\"xyz_t \",xyz_t.shape)\n",
    "        # print(\"t1d \",t1d.shape)\n",
    "        # print(\"xyz_prev \",xyz_prev.shape)\n",
    "        # print(\"unclamp \",unclamp)\n",
    "        # print(\"atom_mask \",atom_mask.shape)\n",
    "        # print('same chain ',same_chain.shape)\n",
    "        mask_dict = generate_masks(msa, task, self.params, chosen_dataset, complete_chain)\n",
    "\n",
    "        return seq, msa, msa_masked, msa_full, mask_msa, true_crds, atom_mask, idx_pdb, xyz_t, t1d, xyz_prev, same_chain, unclamp, negative, mask_dict, task, chosen_dataset, atom_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_conf.pdb_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    rows = [[r[0],r[3],int(r[4]), int(r[-1].strip())] for r in reader\n",
    "            if float(r[2])<=data_conf.resolution_cutoff and\n",
    "            parser.parse(r[1])<=parser.parse(data_conf.date_cutoff) and len(r[-2]) <= data_conf.max_len and len(r[-2]) >= 60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hash = list()\n",
    "train_pdb = {}\n",
    "valid_pdb = {}\n",
    "valid_homo = {}\n",
    "val_clusters = set([int(l) for l in open(data_conf.valid_pdb_dir).readlines()])\n",
    "for r in rows:\n",
    "    if r[2] in val_clusters:\n",
    "        val_hash.append(r[1])\n",
    "        if r[2] in valid_pdb.keys():\n",
    "            valid_pdb[r[2]].append((r[:2], r[-1]))\n",
    "        else:\n",
    "            valid_pdb[r[2]] = [(r[:2], r[-1])]\n",
    "    else:\n",
    "        if r[2] in train_pdb.keys():\n",
    "            train_pdb[r[2]].append((r[:2], r[-1]))\n",
    "        else:\n",
    "            train_pdb[r[2]] = [(r[:2], r[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(OmegaConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_dir = '/projects/ml/TrRosetta'\n",
    "base_dir = os.path.join(os_dir, 'PDB-2021AUG02')\n",
    "base_fb_dir = os.path.join(os_dir, 'fb_af')\n",
    "\n",
    "data_conf = OmegaConf.create({\n",
    "    'resolution_cutoff': 5.0,\n",
    "    'base_dir': base_dir,\n",
    "    'base_fb_dir': base_fb_dir,\n",
    "    'max_len': 260,\n",
    "    'min_len': 60,\n",
    "    'date_cutoff': \"2020-Apr-30\",\n",
    "    'pdb_csv': os.path.join(base_dir, 'list_v02.csv'),\n",
    "    'pdb_dir': os.path.join(base_dir, 'torch/pdb'),\n",
    "    'fb_dir': os.path.join(base_fb_dir, 'pdb'),\n",
    "    'valid_clusters': os.path.join(base_dir, 'val/xaa'),\n",
    "    'fb_csv': os.path.join(base_fb_dir, 'list_b1-3.csv'),\n",
    "    'min_plddt': 80.0,\n",
    "    'min_fb_len': 100,\n",
    "    'cache_path': './pkl_jar/dataset.pkl'\n",
    "})\n",
    "\n",
    "diffusion_conf = OmegaConf.create({\n",
    "    \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution_cutoff: 5.0\n",
      "base_dir: /projects/ml/TrRosetta/PDB-2021AUG02\n",
      "base_fb_dir: /projects/ml/TrRosetta/fb_af\n",
      "max_len: 260\n",
      "min_len: 60\n",
      "date_cutoff: 2020-Apr-30\n",
      "pdb_csv: /projects/ml/TrRosetta/PDB-2021AUG02/list_v02.csv\n",
      "pdb_dir: /projects/ml/TrRosetta/PDB-2021AUG02/torch/pdb\n",
      "fb_dir: /projects/ml/TrRosetta/fb_af/pdb\n",
      "valid_clusters: /projects/ml/TrRosetta/PDB-2021AUG02/val/xaa\n",
      "fb_csv: /projects/ml/TrRosetta/fb_af/list_b1-3.csv\n",
      "min_plddt: 80.0\n",
      "min_fb_len: 100\n",
      "cache_path: ./pkl_jar/dataset.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(data_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OmegaConf.to_yaml(data_conf) == OmegaConf.to_yaml(data_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_pdb_csv(*, csv_path, max_len, min_len, res_cutoff, date_cutoff):\n",
    "    \"\"\"Parse and filter PDB metadata csv.\"\"\"\n",
    "    raw_pdb_csv = pd.read_csv(data_conf.pdb_csv)\n",
    "    raw_pdb_csv['seq_len'] = raw_pdb_csv.SEQUENCE.apply(lambda x: len(x))\n",
    "    raw_pdb_csv['resolution'] = raw_pdb_csv.RESOLUTION.apply(lambda x: float(x))\n",
    "    raw_pdb_csv['date'] = raw_pdb_csv.DEPOSITION.apply(lambda x: parser.parse(x))\n",
    "    raw_pdb_csv['source'] = 'pdb'\n",
    "    date_cutoff = parser.parse(date_cutoff)\n",
    "    return raw_pdb_csv[\n",
    "        (raw_pdb_csv.seq_len <= max_len) &\n",
    "        (min_len <= raw_pdb_csv.seq_len) &\n",
    "        (raw_pdb_csv.resolution <= res_cutoff) &\n",
    "        (raw_pdb_csv.date <= date_cutoff)\n",
    "    ]\n",
    "\n",
    "def _parse_fb_csv(*, csv_path, max_len, min_len, min_plddt):\n",
    "    \"\"\"Parse and filter FB metadata csv.\"\"\"\n",
    "    raw_fb_csv = pd.read_csv(csv_path).rename(\n",
    "        {'#CHAINID': 'CHAINID'}, axis='columns')\n",
    "    raw_fb_csv['seq_len'] = raw_fb_csv.SEQUENCE.apply(lambda x: len(x))\n",
    "    raw_fb_csv['source'] = 'fb'\n",
    "    return raw_fb_csv[\n",
    "        (raw_fb_csv.plDDT >= min_plddt) &\n",
    "        (raw_fb_csv.seq_len <= max_len) &\n",
    "        (min_len <= raw_fb_csv.seq_len)\n",
    "    ]\n",
    "\n",
    "def _parse_clusters(df):\n",
    "    processed_clusters = {}\n",
    "    for cluster_id, cluster_df in tqdm(df.groupby('CLUSTER')):\n",
    "        num_seqs = len(cluster_df)\n",
    "        avg_len = cluster_df.seq_len.sum() // num_seqs\n",
    "        weight = (1 / 512.) * max(min(float(avg_len), 512.), 256.)\n",
    "        membership = cluster_df.CHAINID.tolist()\n",
    "        processed_clusters[cluster_id] = (\n",
    "            cluster_id,\n",
    "            num_seqs,\n",
    "            avg_len,\n",
    "            weight,\n",
    "            membership\n",
    "        )\n",
    "    return processed_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDB data into train and validation.\n",
    "pdb_csv = _parse_pdb_csv(\n",
    "    csv_path=data_conf.pdb_csv,\n",
    "    max_len=data_conf.max_len,\n",
    "    min_len=data_conf.min_len,\n",
    "    res_cutoff=data_conf.resolution_cutoff,\n",
    "    date_cutoff=data_conf.date_cutoff,\n",
    ")\n",
    "\n",
    "fb_csv = _parse_fb_csv(\n",
    "    csv_path=data_conf.fb_csv,\n",
    "    max_len=data_conf.max_len,\n",
    "    min_len=data_conf.min_fb_len,\n",
    "    min_plddt=data_conf.min_plddt,\n",
    ")\n",
    "\n",
    "# Assign splits\n",
    "val_clusters = set([int(l) for l in open(data_conf.valid_clusters).readlines()])\n",
    "pdb_csv['split'] = pdb_csv.CLUSTER.apply(lambda x: 'valid' if x in val_clusters else 'train')\n",
    "train_pdb_csv = pdb_csv[pdb_csv.split == 'train']\n",
    "valid_pdb_csv = pdb_csv[pdb_csv.split == 'valid']\n",
    "fb_csv['split'] = fb_csv.CLUSTER.apply(lambda x: 'valid' if x in val_clusters else 'train')\n",
    "train_fb_csv = fb_csv[fb_csv.split == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1550590/1550590 [05:04<00:00, 5084.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Parse train clusters\n",
    "train_csv = pd.concat([pdb_csv, fb_csv])\n",
    "train_clusters = _parse_clusters(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 251 ms, sys: 0 ns, total: 251 ms\n",
      "Wall time: 254 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAINID</th>\n",
       "      <th>DEPOSITION</th>\n",
       "      <th>RESOLUTION</th>\n",
       "      <th>HASH</th>\n",
       "      <th>CLUSTER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LEN_EXIST</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>resolution</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>plDDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5naj_A</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>1.46</td>\n",
       "      <td>30830</td>\n",
       "      <td>22021</td>\n",
       "      <td>GSMSEQSICQARAAVMVYDDANKKWVPAGGSTGFSRVHIYHHTGNN...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>113</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>pdb</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHAINID  DEPOSITION  RESOLUTION   HASH  CLUSTER  \\\n",
       "9  5naj_A  2017-02-28        1.46  30830    22021   \n",
       "\n",
       "                                            SEQUENCE  LEN_EXIST  seq_len  \\\n",
       "9  GSMSEQSICQARAAVMVYDDANKKWVPAGGSTGFSRVHIYHHTGNN...      110.0      113   \n",
       "\n",
       "   resolution       date source  split  plDDT  \n",
       "9        1.46 2017-02-28    pdb  train    NaN  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_csv[train_csv.CHAINID == '5naj_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([      9,      10,      11,      12,      15,      16,      17,\n",
       "                 20,      21,      29,\n",
       "            ...\n",
       "            7597978, 7597979, 7597980, 7597981, 7597982, 7597985, 7597986,\n",
       "            7597988, 7597989, 7597994],\n",
       "           dtype='int64', length=4334387)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "precomputed_data = (\n",
    "    data_conf,\n",
    "    train_csv,\n",
    "    train_clusters,\n",
    "    valid_pdb_csv,\n",
    ")\n",
    "with open(data_conf.cache_path, \"wb\") as f:\n",
    "    pickle.dump(precomputed_data, f)\n",
    "    print ('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_conf.cache_path, \"rb\") as f:\n",
    "    (\n",
    "        data_conf,\n",
    "        train_csv,\n",
    "        train_clusters,\n",
    "        valid_pdb_csv,\n",
    "    ) = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdb(chain_id):\n",
    "    parsed_pdb = torch.load(\n",
    "        os.path.join(data_conf.pdb_dir, chain_id[1:3], f'{chain_id}.pt'))\n",
    "    xyz = parsed_pdb['xyz']\n",
    "    num_res = len(xyz)\n",
    "    mask = parsed_pdb['mask'].long()\n",
    "    res_idx = torch.arange(num_res).long()\n",
    "    xyz = torch.nan_to_num(xyz).float()\n",
    "    seq = torch.tensor([residue_constants.restype_order_with_x[i] for i in parsed_pdb['seq']]).long()    \n",
    "    # TODO: Remove beginning and ending tags\n",
    "    res_mask = torch.any(mask, axis=-1)\n",
    "    feats = {\n",
    "        'xyz': xyz,\n",
    "        'mask': mask,\n",
    "        'res_mask': res_mask,\n",
    "        'seq': seq,\n",
    "        'res_idx': res_idx,\n",
    "    }\n",
    "    return chain_id, feats\n",
    "\n",
    "def load_fb(chain_id, chain_hash):\n",
    "    file_path = os.path.join(\n",
    "        data_conf.fb_dir, chain_hash[:2], chain_hash[2:], chain_id)\n",
    "    xyz, mask, res_idx, seq = parse_pdb(file_path+'.pdb')\n",
    "    plddt = np.load(file_path+'.plddt.npy')\n",
    "    # TODO: Add back sidechain pLDDT masking\n",
    "    mask = np.logical_and(mask, (plddt > data_conf.min_plddt)[:,None])\n",
    "    seq = np.array([residue_constants.restype_order_with_x[i] for i in seq])\n",
    "    # TODO: Remove beginning and ending tags\n",
    "    res_mask = np.any(mask, axis=-1)\n",
    "    feats = {\n",
    "        'xyz': torch.tensor(xyz).float(),\n",
    "        'mask': torch.tensor(mask).long(),\n",
    "        'res_mask': torch.tensor(res_mask).long(),\n",
    "        'seq': torch.tensor(seq).long(),\n",
    "        'res_idx': torch.tensor(res_idx).long(),\n",
    "    }\n",
    "    return os.path.basename(file_path), feats\n",
    "\n",
    "def parse_pdb(filename):\n",
    "    lines = open(filename,'r').readlines()\n",
    "    return parse_pdb_lines(lines)\n",
    "\n",
    "def parse_pdb_lines(lines):\n",
    "\n",
    "    # indices of residues observed in the structure\n",
    "    idx_s = [int(l[22:26]) for l in lines if l[:4]==\"ATOM\" and l[12:16].strip()==\"CA\"]\n",
    "\n",
    "    # 4 BB + up to 10 SC atoms\n",
    "    xyz = np.full((len(idx_s), 14, 3), np.nan, dtype=np.float32)\n",
    "    seq = []\n",
    "    for l in lines:\n",
    "        if l[:4] != \"ATOM\":\n",
    "            continue\n",
    "        resNo, atom, aa = int(l[22:26]), l[12:16], l[17:20]\n",
    "        seq.append(residue_constants.restype_3to1[aa])\n",
    "        idx = idx_s.index(resNo)\n",
    "        for i_atm, tgtatm in enumerate(chemical.aa2long[chemical.aa2num[aa]]):\n",
    "            if tgtatm == atom:\n",
    "                xyz[idx,i_atm,:] = [float(l[30:38]), float(l[38:46]), float(l[46:54])]\n",
    "                break\n",
    "\n",
    "    # save atom mask\n",
    "    mask = np.logical_not(np.isnan(xyz[...,0]))\n",
    "    xyz[np.isnan(xyz[...,0])] = 0.0\n",
    "\n",
    "    return xyz, mask, np.array(idx_s), ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_id, pdb_data = load_pdb('5naj_A')\n",
    "fb_id, fb_data = load_fb('UniRef50_A0A1N6PI58', 'c87b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(pdb_data['res_mask'].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7842)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(fb_data['res_mask'].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAINID</th>\n",
       "      <th>DEPOSITION</th>\n",
       "      <th>RESOLUTION</th>\n",
       "      <th>HASH</th>\n",
       "      <th>CLUSTER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LEN_EXIST</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>resolution</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>plDDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6977295</th>\n",
       "      <td>UniRef50_A0A1H3MGD0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dda8</td>\n",
       "      <td>2660271</td>\n",
       "      <td>MKNIRFYEAEKYNSDDYEKVEDMIYMPHHDPSEQNIIYVTSIIYEP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>fb</td>\n",
       "      <td>train</td>\n",
       "      <td>94.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     CHAINID DEPOSITION  RESOLUTION  HASH  CLUSTER  \\\n",
       "6977295  UniRef50_A0A1H3MGD0        NaN         NaN  dda8  2660271   \n",
       "\n",
       "                                                  SEQUENCE  LEN_EXIST  \\\n",
       "6977295  MKNIRFYEAEKYNSDDYEKVEDMIYMPHHDPSEQNIIYVTSIIYEP...        NaN   \n",
       "\n",
       "         seq_len  resolution date source  split  plDDT  \n",
       "6977295      128         NaN  NaT     fb  train  94.29  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_weights, pdb_splits, pdb_sizes = _calc_cluster_weights(pdb_csv)\n",
    "fb_weights, fb_splits, fb_sizes = _calc_cluster_weights(fb_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DistilledDataset(pdb_IDs, loader_pdb, loader_pdb_fixbb, pdb_dict,\n",
    "                             compl_IDs, loader_complex, loader_complex_fixbb, compl_dict,\n",
    "                             neg_IDs, loader_complex, neg_dict,\n",
    "                             fb_IDs, loader_fb, loader_fb_fixbb, fb_dict,\n",
    "                             homo, self.loader_param)\n",
    "\n",
    "valid_pdb_set = Dataset(list(valid_pdb.keys())[:self.n_valid_pdb],\n",
    "                        loader_pdb, valid_pdb,\n",
    "                        self.loader_param, homo, p_homo_cut=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
